{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install mimikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ktonal/mimikit@experiment/new-data-again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cd into project's repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd AIMC-Submission-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "\n",
    "from mimikit.models.sample_rnn import SampleRNN\n",
    "from mimikit import get_trainer, audio, NeptuneConnector\n",
    "from mimikit.audios import transforms as A\n",
    "\n",
    "nc = NeptuneConnector(user=\"k-tonal\",\n",
    "                      setup={\n",
    "                          \"model\": \"experiment-Stimme\",\n",
    "                          \"trained\": \"experiment-Stimme/EXS-1\"\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make db from list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbs import DBS\n",
    "\n",
    "# Add your files combinations in dbs.py and load them here\n",
    "\n",
    "DBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  Options for creating DBS ###########\n",
    "\n",
    "# for instance :\n",
    "\n",
    "files = DBS[\"throat\"]\n",
    "\n",
    "# or when you downloaded a model :\n",
    "\n",
    "# files = net.hparams.files\n",
    "\n",
    "# or directly so :\n",
    "\n",
    "# files = [\"Laura Newton.m4a\", \"Perotin.mp3\", \"Stimmung.mp3\"]\n",
    "\n",
    "###################################################\n",
    "\n",
    "files_paths = [\"./data/\" + file for file in files]\n",
    "\n",
    "db = SampleRNN.db_class.make(\"/content/tmp-db.h5\", files=files_paths, sr=16000, mu=255)\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.download_experiment(\"trained\", destination=\"/content/sample_rnn/\", artifacts=\"states/\")\n",
    "\n",
    "net = SampleRNN.load_from_checkpoint(\"/content/sample_rnn/\" + nc.setup[\"trained\"].split(\"/\")[-1] + \"/states/epoch=2.ckpt\", db=db)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model's args\n",
    "\n",
    "\n",
    "- the [original paper](https://arxiv.org/pdf/1612.07837.pdf) has some recommendations, and the [Dadabots](http://dadabots.com/nips2017/generating-black-metal-and-math-rock.pdf) made a pretty good job at describing their experiments.\n",
    "\n",
    "\n",
    "- the most important arg is `frame_sizes` (see [the original repo](https://github.com/soroushmehr/sampleRNN_ICLR2017) for a visual aid to this)\n",
    "\n",
    "    - `SampleRNN` doesn't have \"layers\", it has \"tiers\" : small models that process `frame_size` inputs at a time and combine their outputs with the outputs of the previous tier.\n",
    "    \n",
    "    - `frame_sizes` argument determine how many samples each tier (from top to bottom!) processes at a time. The repo's image corresponds to `frame_sizes=(16, 4, 4)` for tier 3, 2, 1, which does, in fact work pretty well...\n",
    "    \n",
    "    - **IMPORTANT!** you can have as many tiers as you want, but :\n",
    "    \n",
    "        1. the two last tiers must have the same `frame_size`\n",
    "        \n",
    "        2. dividing a tier's frame_size by the next tier's frame_size should always result in an exact integer. e.g. \n",
    "            - (128, 4, 4) => **yes** because 128 / 4 == 32\n",
    "            - (12, 11, 11) => **no** because 12 / 11 == 1.0909090909090908\n",
    "            \n",
    "        3. The first frame_size has to be smaller or equal to an other arg : `batch_seq_length`\n",
    "\n",
    "    - the original paper says `(8, 2, 2)` worked best. Dadabots used only 2 tiers, probably `(4, 4)` or similar. With this implementation you could go wild and do `(256, 128, 64, 32, 16, 8, 4, 2, 2)` or even more...\n",
    "    \n",
    "\n",
    "- Each tier but the last has a Recurrent Network with 1 or more layers. The `n_rnn` argument specifies how many layers **per tier**. It seems to me that it starts working when the whole model has a total of at least 4 rnns : e.g. `frame_sizes=(8, 2, 2)` & `n_rnn=2` corresponds to 4 rnns total (last tier always has 0 rnns). Dadabots made their streams with 2 tiers and the top tier had between 5 and 9 rnns...\n",
    "\n",
    "\n",
    "- `*_dim` arguments are very similar to `model_dim` in `FreqNet`. \n",
    "\n",
    "    - `net_dim` is the most important and will greatly influence the trade-off between speed & expressivity. It could have been named `model_dim` because most of the network's parameters will have sizes proportional to `net_dim`. `512` works well. Maybe you can go down to `256` for more speed or up to `1024` for more expressivity... Definitely worths playing with!\n",
    "    \n",
    "    - `emb_dim` is just for a few parameters and might not be very important. `256` works, but I would expect so would `128` or `64`, maybe even `32`... More than `256` could be too much but, honnestly, IDUNO!... :)\n",
    "    \n",
    "    - `mlp_dim` is for the tipp of the model (which makes the prediction). `512` works. Once again, I'm not sure how relevant this `dim` is...\n",
    "\n",
    "\n",
    "- `max_epochs` : it seems SampleRNN generates quite well very early! Values for the loss that resulted in cool outputs for me were around 1.6 to 1.9 and this was after just a few epochs! It seems even that training too long results in long silent outputs, this happened to me after 100 epochs and a loss around 1.4.\n",
    "\n",
    "\n",
    "- `max_lr` : it also seems that SampleRNN withstands high learning rates, which also means faster training! As a comparaison, freqnet starts to diverge with `max_lr > 1e-3` but here `5e-3` works, even if it's probably already at the limit... If the loss starts to increase, fall back to `max_lr=1e-3` and you should be fine.\n",
    "    \n",
    "    \n",
    "- the values used in the next cell seem to work quite well. In doubt, use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SampleRNN(db=db, \n",
    "                frame_sizes=(16, 4, 4),\n",
    "                net_dim=512,\n",
    "                emb_dim=256,\n",
    "                mlp_dim=512,\n",
    "                n_rnn=2,\n",
    "                max_lr=1e-3,\n",
    "                div_factor=2.,\n",
    "                betas=(.9, .9),\n",
    "                batch_size=128,\n",
    "                batch_seq_len=512,\n",
    "                ##### params for monitoring : #######\n",
    "                test_every_n_epochs=2,\n",
    "                n_test_warmups=10,\n",
    "                n_test_prompts=2,\n",
    "                n_test_steps=16000,\n",
    "                test_temp=0.5,\n",
    "               )\n",
    "\n",
    "net.hparams.files = files\n",
    "net.hparams.model_class = \"SampleRNN\"\n",
    "\n",
    "net.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = get_trainer(root_dir=\"/content/sample_rnn\",\n",
    "                      max_epochs=100,\n",
    "                      epochs=[10, 25, 49],\n",
    "                     # comment these if you don't want to track with neptune :\n",
    "                     model=net,\n",
    "                     neptune_connector=nc,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(net)\n",
    "\n",
    "nc.upload_model(\"model\", net, artifacts=(\"states\", ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Generating with SampleRNN is quite flexible!\n",
    "\n",
    "1. The model has an internal state that is suppose to encode what the model has seen _until now_. So before we let him run loose, we can \"warm up\" the model with a prompt of `n_warmups` batches. (I'm not sure if it changes much for the outputs but it's worth playing with...)\n",
    "\n",
    "\n",
    "2. The generation method has 2 modes : deterministic and probabilistic. In the first, the output will always be the same for the same prompt/warm-up. But the second mode samples the outputs from probabilities that can be modified with a very interesting parameter : `temperature`.\n",
    "    - `temperature` must be bigger than 0. and altough it could theoretically be greater than 1., values above 1. might not be so interesting because :\n",
    "    - the higher the `temperature`, the \"noisier\" the output. The lower the temp, the more \"frozen\" the output. It is called \"temperature\" because it corresponds to some heat equations : more heat = particles move faster, less heat = particles stop moving. Musically, it means : hotter = more contrasts, cooler = longer sounds.\n",
    "    - Concretly, I recommend starting around `temperature=0.5` and going tiny bits up & down....\n",
    "    \n",
    "    \n",
    "3. Because generating in time-domain is much slower than in freq-dom, **generation is split in 2 cells**:\n",
    "    - the first gets a **new prompt** and do some warmups\n",
    "    - the second generates and **appends** the results to what has been previously generated.\n",
    "This way, you can evaluate the first once and evaluate the 2nd several times. This beats waiting 30min to discover that it generated 30 seconds of silence...\n",
    "\n",
    "\n",
    "4. You can generate `n_prompts` at the same time (like in redundance rate). This is much faster than generating one prompt at a time.\n",
    "\n",
    "\n",
    "5. Because feeding data to SampleRNN is a bit complex, you'll have to stick to random prompts from the training data for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARM-UP\n",
    "\n",
    "n_prompts = 8\n",
    "n_warmups = 20\n",
    "\n",
    "\n",
    "new = net.warm_up(n_warmups, n_prompts)\n",
    "        \n",
    "new.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-evaluate the next cell for generating further with the same prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########### PLAY WITH THOSE : ###################\n",
    "\n",
    "# to use the deterministic mode, set temperature to None\n",
    "\n",
    "temperature = 0.5\n",
    "\n",
    "# 1 second = 16000 steps !\n",
    "\n",
    "n_steps = 32000\n",
    "\n",
    "###############################################\n",
    "\n",
    "## LOS GEHT'S!\n",
    "\n",
    "new = net.generate(new, n_steps, decode_outputs=True, temperature=temperature)\n",
    "\n",
    "\n",
    "for i in range(new.size(0)):\n",
    "\n",
    "    y = new[i].squeeze().cpu().numpy()\n",
    "\n",
    "    print(\"prompt number\", i)\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(y)\n",
    "    plt.show()\n",
    "\n",
    "    audio(y, sr=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log selected prompts to neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "from random import randint\n",
    "\n",
    "numbers_to_log = [0, 3, 5]\n",
    "\n",
    "for i in numbers_to_log:\n",
    "    y = new[i].squeeze().cpu().numpy()\n",
    "    \n",
    "    net.log_audio(\"output_id%i\" % randint(0, 1e5), y, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
